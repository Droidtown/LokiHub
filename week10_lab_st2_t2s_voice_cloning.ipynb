{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zoew-r/LokiHub/blob/main/week10_lab_st2_t2s_voice_cloning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f651dd18",
      "metadata": {
        "id": "f651dd18"
      },
      "source": [
        "# 第10週實驗課：語音轉文字、大型語言模型與文字轉語音（含聲音複製）\n",
        "\n",
        "本次實驗課目標：\n",
        "1.  **語音轉文字 (S2T)**：將聲音轉成文字。\n",
        "2.  **大型語言模型 (LLM)**：用來修正文字文法。\n",
        "3.  **文字轉語音 (TTS) 與聲音複製**：將文字轉成特定人聲的語音。\n",
        "\n",
        "我們會先分別測試，再整合成一個專案。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abb8bd64",
      "metadata": {
        "id": "abb8bd64"
      },
      "source": [
        "## 安裝所需套件\n",
        "\n",
        "執行此區塊來安裝本次實驗所需的 Python 套件。\n",
        "`%capture` 會隱藏安裝過程的輸出訊息。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78719d6a",
      "metadata": {
        "id": "78719d6a",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%capture install_logs\n",
        "!apt install ffmpeg\n",
        "!pip install autoawq ffmpeg wavio\n",
        "!pip install gradio\n",
        "!pip install f5-tts # https://github.com/SWivid/F5-TTS/tree/main"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5119d66a",
      "metadata": {
        "id": "5119d66a"
      },
      "source": [
        "## 測試文字轉語音 (TTS) Gradio 介面\n",
        "\n",
        "`f5-tts` 提供了一個網頁介面方便快速測試。取消下一行的註解並執行，會產生一個公開網址。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70a56d37",
      "metadata": {
        "id": "70a56d37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e47967b5-9d1a-4e0e-a341-272e3ff4fecd"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-11-06 02:27:22.284637: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1762396042.304895    5330 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1762396042.310987    5330 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1762396042.326296    5330 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762396042.326319    5330 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762396042.326324    5330 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762396042.326329    5330 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-06 02:27:22.330905: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.12/dist-packages/jieba/__init__.py:44: SyntaxWarning: invalid escape sequence '\\.'\n",
            "  re_han_default = re.compile(\"([\\u4E00-\\u9FD5a-zA-Z0-9+#&\\._%\\-]+)\", re.U)\n",
            "/usr/local/lib/python3.12/dist-packages/jieba/__init__.py:46: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  re_skip_default = re.compile(\"(\\r\\n|\\s)\", re.U)\n",
            "/usr/local/lib/python3.12/dist-packages/jieba/finalseg/__init__.py:78: SyntaxWarning: invalid escape sequence '\\.'\n",
            "  re_skip = re.compile(\"([a-zA-Z0-9]+(?:\\.\\d+)?%?)\")\n",
            "Download Vocos from huggingface charactr/vocos-mel-24khz\n",
            "config.yaml: 100% 461/461 [00:00<00:00, 3.88MB/s]\n",
            "pytorch_model.bin: 100% 54.4M/54.4M [00:00<00:00, 65.4MB/s]\n",
            "F5TTS_v1_Base/model_1250000.safetensors: 100% 1.35G/1.35G [00:11<00:00, 113MB/s]\n",
            "\n",
            "vocab :  /usr/local/lib/python3.12/dist-packages/f5_tts/infer/examples/vocab.txt\n",
            "token :  custom\n",
            "model :  /root/.cache/huggingface/hub/models--SWivid--F5-TTS/snapshots/84e5a410d9cead4de2f847e7c9369a6440bdfaca/F5TTS_v1_Base/model_1250000.safetensors \n",
            "\n",
            "Starting app...\n",
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://6535b310724fe6d2c1.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        }
      ],
      "source": [
        "!f5-tts_infer-gradio --share # 啟動 Gradio 介面"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8827369",
      "metadata": {
        "id": "f8827369"
      },
      "source": [
        "## 匯入所需模組\n",
        "\n",
        "匯入程式中會用到的 Python 模組。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c51e19f2",
      "metadata": {
        "id": "c51e19f2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
        "import functools\n",
        "import json\n",
        "from pathlib import Path\n",
        "from pprint import pprint\n",
        "import shlex\n",
        "import subprocess\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore') # 忽略警告訊息\n",
        "\n",
        "import gdown # 從 Google Drive 下載\n",
        "import gradio as gr\n",
        "from huggingface_hub import notebook_login # Hugging Face 登入\n",
        "import IPython # 顯示音訊播放器等\n",
        "import numpy as np\n",
        "from rich import print as rprint\n",
        "import torch # PyTorch 深度學習框架\n",
        "from transformers import ( # Hugging Face 函式庫\n",
        "    AutoProcessor, AutoTokenizer, TextGenerationPipeline,\n",
        "    AutoModelForSpeechSeq2Seq, AutomaticSpeechRecognitionPipeline,\n",
        "    pipeline, BitsAndBytesConfig\n",
        ")\n",
        "import wavio\n",
        "\n",
        "# 檢查是否有 GPU 可用\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"使用的運算裝置 (Device): {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "077935bb",
      "metadata": {
        "id": "077935bb"
      },
      "source": [
        "## Hugging Face Hub 登入 (非必要)\n",
        "\n",
        "若要使用需要授權的模型 (如 Llama 3)，請執行 `notebook_login()`。若只用公開模型則跳過。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a78bda8a",
      "metadata": {
        "id": "a78bda8a"
      },
      "outputs": [],
      "source": [
        "# notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bed38391",
      "metadata": {
        "id": "bed38391"
      },
      "source": [
        "## 下載範例聲音檔案\n",
        "\n",
        "從 Google Drive 下載包含多個角色聲音的範例檔案夾。\n",
        "`!ls` 用來確認檔案已下載。\n",
        "\n",
        "* Trump: https://huggingface.co/datasets/tuenguyen/trump-speech-dataset-tts\n",
        "* Arnold: https://www.youtube.com/watch?v=_wyLd0HUK04\n",
        "* Donald Duck: https://www.youtube.com/watch?v=5lDdJOjU92A\n",
        "* G Dragon: https://youtu.be/3xE4RTrghEI?si=pEtwbSXrdoZiyBlH\n",
        "* SpongeBob: https://www.youtube.com/watch?v=kkAXjk-cJ5M\n",
        "* Christopher Walken: https://www.youtube.com/watch?v=4DI527Uf2Q0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11818a5d",
      "metadata": {
        "id": "11818a5d"
      },
      "outputs": [],
      "source": [
        "url = \"https://drive.google.com/drive/folders/1dq-koI-P-tYJDfE1DtLdNZrUdCJLwcQZ?usp=sharing\"\n",
        "gdown.download_folder(url, quiet=True, use_cookies=False)\n",
        "!ls week10/ # 列出下載的資料夾內容"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70572f5e",
      "metadata": {
        "id": "70572f5e"
      },
      "source": [
        "## 語音轉文字 (Speech-to-Text, S2T)\n",
        "\n",
        "使用 [OpenAI Whisper](https://huggingface.co/openai/whisper-large-v3-turbo) 模型將聲音轉成文字。\n",
        "`prepare_whisper_model` 函式會載入模型並設定好 pipeline。\n",
        "`@functools.cache` 用於快取模型，避免重複載入。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe163b8c",
      "metadata": {
        "id": "fe163b8c"
      },
      "outputs": [],
      "source": [
        "@functools.cache\n",
        "def prepare_whisper_model(model_id: str = \"openai/whisper-large-v3-turbo\") -> AutomaticSpeechRecognitionPipeline:\n",
        "    \"\"\"載入並設定 Whisper S2T 模型 pipeline。\"\"\"\n",
        "    print(f\"正在載入 Whisper 模型: {model_id} ...\")\n",
        "    torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "    model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
        "        model_id,\n",
        "        torch_dtype=torch_dtype,\n",
        "        low_cpu_mem_usage=True,\n",
        "        attn_implementation=\"sdpa\" # 使用優化的注意力機制\n",
        "    )\n",
        "    model.to(device)\n",
        "    processor = AutoProcessor.from_pretrained(model_id)\n",
        "    pipe = pipeline(\n",
        "        \"automatic-speech-recognition\",\n",
        "        model=model,\n",
        "        tokenizer=processor.tokenizer,\n",
        "        feature_extractor=processor.feature_extractor,\n",
        "        chunk_length_s=30, # 處理 30 秒聲音片段\n",
        "        batch_size=16,     # 可依 GPU 記憶體調整\n",
        "        torch_dtype=torch_dtype,\n",
        "        device=device,\n",
        "    )\n",
        "    print(\"Whisper 模型載入完成。\")\n",
        "    return pipe\n",
        "\n",
        "# 載入 S2T 模型\n",
        "s2t = prepare_whisper_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c971305",
      "metadata": {
        "id": "8c971305"
      },
      "source": [
        "### 執行語音轉文字\n",
        "\n",
        "將 `week10` 資料夾中的第一個 `.wav` 檔進行辨識。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "565d7e5a",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "565d7e5a"
      },
      "outputs": [],
      "source": [
        "# 取得 week10 資料夾中所有 wav 檔案路徑\n",
        "audio_files = list(Path(\"./week10\").glob(\"*.wav\"))\n",
        "if not audio_files:\n",
        "    print(\"錯誤：在 week10 資料夾中找不到任何 .wav 檔案。\")\n",
        "else:\n",
        "    first_audio_path = str(audio_files[0].resolve())\n",
        "    print(\"找到的聲音檔案路徑 (部分):\")\n",
        "    rprint([str(p) for p in audio_files[:5]]) # 只印出前 5 個\n",
        "\n",
        "    print(f\"\\n播放第一個聲音檔: {first_audio_path}\")\n",
        "    display(IPython.display.Audio(first_audio_path))\n",
        "\n",
        "    print(\"\\n正在進行語音轉文字...\")\n",
        "    transcription = s2t(first_audio_path) # 對第一個檔案進行辨識\n",
        "    print(\"\\n辨識結果:\")\n",
        "    rprint(transcription)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "569ea8af",
      "metadata": {
        "id": "569ea8af"
      },
      "source": [
        "## 大型語言模型 (LLM) - 文法修正\n",
        "\n",
        "使用 LLM (例如 [Qwen2.5-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-AWQ)) 來修正 S2T 結果的文法錯誤。\n",
        "`prepare_llm` 函式載入 LLM pipeline。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa39699a",
      "metadata": {
        "id": "aa39699a"
      },
      "outputs": [],
      "source": [
        "@functools.cache\n",
        "def prepare_llm(model_id: str = \"Qwen/Qwen2.5-14B-Instruct-AWQ\") -> TextGenerationPipeline:\n",
        "    \"\"\"載入並設定 LLM 文字生成 pipeline。\"\"\"\n",
        "    print(f\"正在載入 LLM: {model_id} ...\")\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model_id,\n",
        "        torch_dtype=torch.float16,\n",
        "    )\n",
        "    print(\"LLM 載入完成。\")\n",
        "    return pipe\n",
        "\n",
        "# 載入 LLM\n",
        "llm = prepare_llm()\n",
        "tokenizer = llm.tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44da60e7",
      "metadata": {
        "id": "44da60e7"
      },
      "source": [
        "### 測試 LLM 文法修正\n",
        "\n",
        "準備中英文錯誤句子範例，並定義一個函式 `prepare_grammar_correction_messages` 來建立 LLM 的輸入。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65fe65af",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "65fe65af"
      },
      "outputs": [],
      "source": [
        "# 英文錯誤範例\n",
        "english_sentences = [\n",
        "    {\"incorrect\": \"Their going too the park later.\", \"correct\": \"They're going to the park later.\"},\n",
        "    {\"incorrect\": \"Me and him discussed it.\", \"correct\": \"He and I discussed it.\"},\n",
        "]\n",
        "# 中文錯誤範例 (臺灣用語)\n",
        "chinese_sentences = [\n",
        "    {\"incorrect\": \"我明天在去。\", \"correct\": \"我明天再去。\"},\n",
        "    {\"incorrect\": \"這見衣服必較好看。\", \"correct\": \"這件衣服比較好看。\"},\n",
        "]\n",
        "\n",
        "def prepare_grammar_correction_messages(text: str) -> list[dict]:\n",
        "    \"\"\"建立 LLM 文法修正任務的 messages 列表。\"\"\"\n",
        "    system_message = {\"role\": \"system\", \"content\": \"你是一位專業的編輯，擅長修正英文和繁體中文的文法與拼寫錯誤。請直接提供修正後的文字，不要包含任何解釋。\"}\n",
        "    user_message = {\"role\": \"user\", \"content\": f\"請修正以下文字的文法與拼寫錯誤：\\n```\\n{text}\\n```\"}\n",
        "    return [system_message, user_message]\n",
        "\n",
        "# --- 測試英文 ---\n",
        "print(\"--- 測試英文修正 ---\")\n",
        "text_en_incorrect = english_sentences[0][\"incorrect\"]\n",
        "print(f\"原始: {text_en_incorrect}\")\n",
        "messages_en = prepare_grammar_correction_messages(text_en_incorrect)\n",
        "res_en = llm(messages_en, max_new_tokens=50, return_full_text=False)\n",
        "print(f\"修正: {res_en[0]['generated_text'].strip()}\")\n",
        "print(f\"預期: {english_sentences[0]['correct']}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 測試中文 ---\n",
        "print(\"--- 測試中文修正 ---\")\n",
        "text_zh_incorrect = chinese_sentences[1][\"incorrect\"]\n",
        "print(f\"原始: {text_zh_incorrect}\")\n",
        "messages_zh = prepare_grammar_correction_messages(text_zh_incorrect)\n",
        "res_zh = llm(messages_zh, max_new_tokens=50, return_full_text=False)\n",
        "print(f\"修正: {res_zh[0]['generated_text'].strip()}\")\n",
        "print(f\"預期: {chinese_sentences[1]['correct']}\")"
      ],
      "metadata": {
        "id": "M9Uqvmhpg16h"
      },
      "id": "M9Uqvmhpg16h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.apply_chat_template(messages_zh, tokenize=False, add_generation_prompt=True))"
      ],
      "metadata": {
        "id": "nic60htTnu5M"
      },
      "id": "nic60htTnu5M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4d545365",
      "metadata": {
        "id": "4d545365"
      },
      "source": [
        "## 文字轉語音 (TTS) 與聲音複製\n",
        "\n",
        "使用 `f5-tts` 命令列工具，將文字轉換成指定參考聲音 (`ref_audio`) 的語音。\n",
        "`clone_voice` 函式封裝了執行此命令的過程。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0895768d",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "0895768d"
      },
      "outputs": [],
      "source": [
        "def clone_voice(path_to_ref_audio: str,\n",
        "                gen_text: str,\n",
        "                ref_text: str = \"\", # 參考音檔的文字稿 (選填，但建議提供)\n",
        "                output_file: str = \"tts_output.wav\",\n",
        "                output_dir: str = \"tts_output\"): # 改用更有意義的預設目錄\n",
        "    \"\"\"使用 f5-tts 命令列工具進行聲音複製。\"\"\"\n",
        "    cli_executable = \"f5-tts_infer-cli\"\n",
        "    model = \"F5TTS_v1_Base\"\n",
        "    output_path = Path(output_dir) / output_file\n",
        "    output_path.parent.mkdir(parents=True, exist_ok=True) # 建立輸出目錄\n",
        "\n",
        "    command = [\n",
        "        cli_executable, \"--model\", model,\n",
        "        \"--ref_audio\", str(Path(path_to_ref_audio).resolve()),\n",
        "        \"--ref_text\", ref_text,\n",
        "        \"--gen_text\", gen_text,\n",
        "        \"--output_dir\", str(output_path.parent.resolve()),\n",
        "        \"--output_file\", output_path.name,\n",
        "        \"--nfe_step\", \"64\", # 推論步數\n",
        "        \"--device\", device,\n",
        "    ]\n",
        "    print(f\"準備執行 TTS 命令: {shlex.join(command)}\")\n",
        "    try:\n",
        "        result = subprocess.run(command, check=True, capture_output=True, text=True, encoding='utf-8')\n",
        "        print(\"\\nTTS 命令執行成功。\")\n",
        "        print(f\"生成的音訊檔案: {output_path}\")\n",
        "        display(IPython.display.Audio(str(output_path))) # 顯示播放器\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"\\nTTS 命令執行失敗 (錯誤碼 {e.returncode}):\\nstderr: {e.stderr}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"\\n錯誤：找不到命令 '{cli_executable}'。請確認 f5-tts 已安裝。\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n執行 TTS 時發生錯誤: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f6fb19b",
      "metadata": {
        "id": "4f6fb19b"
      },
      "source": [
        "### 測試聲音複製\n",
        "\n",
        "載入範例聲音的文字稿 (`transcripts.json`)，然後選擇一個聲音 (如 Arnold) 來念一段指定的中文文字。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 載入文字稿\n",
        "transcript_file = Path(\"./week10/transcripts.json\")\n",
        "transcripts = {}\n",
        "if transcript_file.exists():\n",
        "    try:\n",
        "        with open(transcript_file, 'r', encoding='utf-8') as f:\n",
        "            transcripts = json.load(f)\n",
        "        print(\"已載入文字稿檔案。\")\n",
        "        print(json.dumps(transcripts, ensure_ascii=False, indent=2))\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"錯誤：無法解析文字稿檔案 {transcript_file}。\")\n",
        "else:\n",
        "    print(f\"警告：找不到文字稿檔案 {transcript_file}。TTS 的 ref_text 將會是空的。\")"
      ],
      "metadata": {
        "id": "p1FuKmWIl0Ly"
      },
      "id": "p1FuKmWIl0Ly",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa930ff3",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "aa930ff3"
      },
      "outputs": [],
      "source": [
        "# --- 選擇聲音並執行 TTS ---\n",
        "voice_key = \"gdragon.wav\" # 選擇要模仿的聲音檔名\n",
        "ref_audio_path_str = f\"./week10/{voice_key}\" # 使用相對路徑更通用\n",
        "ref_audio_path = Path(ref_audio_path_str)\n",
        "\n",
        "if ref_audio_path.exists():\n",
        "    display(IPython.display.Audio(ref_audio_path)) # 播放聲音\n",
        "    ref_transcription = transcripts.get(voice_key, {})\n",
        "    print(ref_transcription)\n",
        "    print(f\"\\n選擇模仿的聲音: {voice_key}\")\n",
        "    if not ref_transcription:\n",
        "        print(\"(警告：找不到此聲音的文字稿，ref_text 將為空)\")\n",
        "\n",
        "    # text_to_generate = \"白日依山盡 黃河入海流 欲窮千里目 更上一層樓。\"\n",
        "    text_to_generate = \"車上有個盆，盆里有個瓶，乓乓乓，乒乒乒，不知是瓶碰盆，還是盆碰瓶。\"\n",
        "    print(f\"要生成的文字: {text_to_generate}\")\n",
        "\n",
        "    clone_voice(\n",
        "        path_to_ref_audio=str(ref_audio_path), # 傳入字串路徑\n",
        "        ref_text=ref_transcription,\n",
        "        gen_text=text_to_generate,\n",
        "        output_file=f\"{voice_key.split('.')[0]}_poem.wav\" # 更有意義的檔名\n",
        "    )\n",
        "else:\n",
        "    print(f\"錯誤：找不到參考聲音檔案 {ref_audio_path_str}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 整合：從輸入到輸出 (課堂練習)\n",
        "\n",
        "`read_for_me` 函式整合了 S2T、LLM 修正、TTS 聲音複製的完整流程。\n",
        "請依照 `FIXME` 的指示，完成函式中缺少的部分。"
      ],
      "metadata": {
        "id": "FGKAJ3A8qhj8"
      },
      "id": "FGKAJ3A8qhj8"
    },
    {
      "cell_type": "code",
      "source": [
        "def read_for_me(\n",
        "    s2t_model: AutomaticSpeechRecognitionPipeline,\n",
        "    llm_model: TextGenerationPipeline,\n",
        "    transcripts_dict: dict, # 改為必要參數\n",
        "    input_text: str | None = None,\n",
        "    input_audio_path: str | None = None,\n",
        "    voice_to_clone_key: str = \"spongebob.wav\", # 預設模仿海綿寶寶\n",
        "    output_file: str = \"final_output.wav\",\n",
        "    output_dir: str = \"tts_final\"\n",
        "):\n",
        "    \"\"\"\n",
        "    整合 S2T -> LLM -> TTS 的流程。\n",
        "    (課堂練習：完成 FIXME 部分)\n",
        "    \"\"\"\n",
        "    # --- 1. 檢查與取得輸入文字 ---\n",
        "    if not input_text and not input_audio_path:\n",
        "        raise ValueError(\"必須提供 input_text 或 input_audio_path。\")\n",
        "    if input_text and input_audio_path:\n",
        "        raise ValueError(\"input_text 和 input_audio_path 不能同時提供。\")\n",
        "    if voice_to_clone_key not in transcripts_dict:\n",
        "        print(f\"警告：在文字稿字典中找不到 key '{voice_to_clone_key}'。\")\n",
        "\n",
        "    original_text = \"\"\n",
        "    if input_audio_path:\n",
        "        print(f\"步驟 1: S2T ({input_audio_path})\")\n",
        "        audio_file = Path(input_audio_path)\n",
        "        if not audio_file.exists(): raise FileNotFoundError(f\"找不到音檔: {input_audio_path}\")\n",
        "\n",
        "\n",
        "        # ----------------------------FIXME---------------------------------\n",
        "        # FIXME 1: 語音轉文字\n",
        "        # 提示：使用 s2t_model 這個 pipeline 來處理聲音檔案。\n",
        "        #      你需要將聲音檔案的路徑 (字串格式，例如 str(audio_file)) 傳遞給 s2t_model。\n",
        "        #      s2t_model 會回傳一個字典，辨識出的文字在 'text' 這個 key 裡面。\n",
        "        #      將辨識出的文字存到 original_text 變數中。\n",
        "        # s2t_result = s2t_model( ... ) # 填入正確的參數\n",
        "        # original_text = s2t_result[ ... ] # 取得 'text'\n",
        "\n",
        "        original_text =\n",
        "        # ----------------------------FIXME---------------------------------\n",
        "\n",
        "        print(f\"  >> S2T 結果: {original_text}\")\n",
        "    else:\n",
        "        original_text = input_text\n",
        "        print(f\"步驟 1: 使用提供的文字: {original_text}\")\n",
        "\n",
        "    # --- 2. LLM 文法修正 ---\n",
        "    print(\"\\n步驟 2: LLM 文法修正\")\n",
        "    # 準備 LLM 輸入訊息 (這部分已完成)\n",
        "    messages = prepare_grammar_correction_messages(original_text)\n",
        "\n",
        "    # ----------------------------FIXME---------------------------------\n",
        "    # FIXME 2: 呼叫 LLM 進行修正\n",
        "    # 提示：使用 llm_model 這個 pipeline 來處理 messages。\n",
        "    #      記得設定 return_full_text=False 只取得模型生成的回應。\n",
        "    #      也可以設定 max_new_tokens (例如 len(original_text) + 50) 來限制輸出長度。\n",
        "    #      llm_model 會回傳一個列表，取第一個元素 (索引為 0) 的字典，\n",
        "    #      其中 'generated_text' key 的值就是修正後的文字。\n",
        "    #      將修正後的文字存到 corrected_text 變數中，並移除前後多餘的空白。\n",
        "    # llm_result = llm_model( ... , return_full_text=False, max_new_tokens=...) # 填入 messages 和其他參數\n",
        "    # corrected_text = llm_result[0][ ... ].strip() # 取得 'generated_text' 並清理\n",
        "\n",
        "    corrected_text =\n",
        "    # ----------------------------FIXME---------------------------------\n",
        "\n",
        "    # 清理常見的模型輸出問題 (例如多餘的引號或標籤)\n",
        "    corrected_text = corrected_text.replace(\"```\", \"\").replace(\"`\", \"\").strip('\"').strip()\n",
        "    print(f\"  >> 修正後文字: {corrected_text}\")\n",
        "\n",
        "    # --- 3. TTS 聲音複製 ---\n",
        "    print(f\"\\n步驟 3: TTS 聲音複製 (模仿 {voice_to_clone_key})\")\n",
        "    # 取得參考聲音的路徑和文字稿 (這部分已完成)\n",
        "    voice_ref_text = transcripts_dict.get(voice_to_clone_key, {})\n",
        "    voice_ref_path_str = f\"./week10/{voice_to_clone_key}\"\n",
        "    voice_ref_path = Path(voice_ref_path_str)\n",
        "\n",
        "    if not voice_ref_path.exists():\n",
        "        raise FileNotFoundError(f\"找不到參考聲音檔: {voice_ref_path_str}\")\n",
        "    if not voice_ref_text:\n",
        "         print(\"(警告：找不到參考聲音文字稿，ref_text 將為空)\")\n",
        "\n",
        "    # ----------------------------FIXME---------------------------------\n",
        "    # FIXME 3: 呼叫聲音複製函式\n",
        "    # 提示：呼叫我們之前定義的 clone_voice 函式。\n",
        "    #      需要傳遞以下參數：\n",
        "    #      - path_to_ref_audio: 參考聲音檔案的路徑 (字串格式，這裡是 voice_ref_path)\n",
        "    #      - gen_text: 要轉換成語音的文字 (這裡是用 LLM 修正過的 corrected_text)\n",
        "    #      - ref_text: 參考聲音的文字稿 (這裡是 voice_ref_text)\n",
        "    #      - output_file: 輸出的檔名 (函式參數 output_file)\n",
        "    #      - output_dir: 輸出的目錄 (函式參數 output_dir)\n",
        "    clone_voice(\n",
        "    #     path_to_ref_audio=str(voice_ref_path),\n",
        "    #     gen_text= ... , # 填入修正後的文字\n",
        "    #     ref_text= ... , # 填入參考文字稿\n",
        "    #     output_file= ... , # 填入輸出檔名\n",
        "    #     output_dir= ...   # 填入輸出目錄\n",
        "    )\n",
        "    # ----------------------------FIXME---------------------------------\n",
        "\n",
        "    # 修正：補上結尾括號\n",
        "    print(f\"\\n流程完成！最終語音預計儲存至 {Path(output_dir) / output_file}\")"
      ],
      "metadata": {
        "id": "QG7hwXUjqu_Q"
      },
      "id": "QG7hwXUjqu_Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "645d2345",
      "metadata": {
        "id": "645d2345"
      },
      "source": [
        "## 執行整合流程\n",
        "\n",
        "測試 `read_for_me` 函式。\n",
        "範例 1 使用文字輸入，範例 2 嘗試使用之前錄製的聲音檔。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "309cf374",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "309cf374"
      },
      "outputs": [],
      "source": [
        "# 執行整合流程 - 範例 1: 文字輸入，海綿寶寶聲音\n",
        "print(\"--- 執行整合流程：範例 1 (文字輸入) ---\")\n",
        "try:\n",
        "    read_for_me(\n",
        "        s2t_model=s2t,\n",
        "        llm_model=llm,\n",
        "        transcripts_dict=transcripts, # 傳入文字稿字典\n",
        "        input_text=\"Me have the most beautiful apples.\", # 簡單錯誤英文\n",
        "        voice_to_clone_key=\"trump.wav\",\n",
        "        output_file=\"trump_corrected_apple.wav\"\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"執行範例 1 時發生錯誤: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 使用 Gradio 錄製你自己的聲音\n",
        "\n",
        "這個區塊會啟動一個簡單的 Gradio 應用程式，讓你可以：\n",
        "1.  點擊「錄音」按鈕開始錄音。\n",
        "2.  再次點擊停止錄音。\n",
        "3.  錄音會自動儲存成 `.wav` 檔案。\n",
        "4.  介面會顯示儲存的檔案路徑，並提供播放和下載的選項。\n",
        "\n",
        "**使用方式：**\n",
        "* 執行這個 Python 程式碼區塊。\n",
        "* 在出現的 Gradio 介面中錄製聲音。\n",
        "* 錄音完成後，複製顯示的 `.wav` 檔案路徑\n",
        "* 你可以在後面的 `read_for_me` 函式呼叫中，將這個路徑作為 `input_audio_path` 參數的值來使用。\n",
        "* **注意：** 錄音完成後，這個 Gradio App 會持續運行。你可以在完成錄音後手動停止這個 Cell 的執行，或者讓它繼續運行直到你關閉 Notebook。"
      ],
      "metadata": {
        "id": "-x2bs_f5zjJk"
      },
      "id": "-x2bs_f5zjJk"
    },
    {
      "cell_type": "code",
      "source": [
        "def save_recording(audio):\n",
        "    \"\"\"\n",
        "    處理 Gradio 音訊輸入，儲存為 WAV 檔案。\n",
        "\n",
        "    Args:\n",
        "        audio: Gradio Audio 元件的回傳值 (設定 type=\"numpy\" 時為 (sample_rate, data) tuple)。\n",
        "\n",
        "    Returns:\n",
        "        儲存的 WAV 檔案路徑。\n",
        "    \"\"\"\n",
        "    if audio is None:\n",
        "        return \"錯誤：未偵測到音訊輸入。\"\n",
        "\n",
        "    sample_rate, data = audio\n",
        "\n",
        "    # 確保 data 是 NumPy array 且為適當的 dtype (例如 int16)\n",
        "    if not isinstance(data, np.ndarray):\n",
        "         return f\"錯誤：音訊資料類型不正確 ({type(data)})，應為 NumPy array。\"\n",
        "\n",
        "    # 將音訊資料轉換為 int16 (常見的 WAV 格式)\n",
        "    # 先檢查最大值以避免 clipping\n",
        "    max_val = np.max(np.abs(data))\n",
        "    if max_val > 0:\n",
        "        data_int16 = (data / max_val * 32767).astype(np.int16)\n",
        "    else:\n",
        "        data_int16 = data.astype(np.int16) # 如果是靜音\n",
        "\n",
        "    # 產生帶有時間戳的檔名\n",
        "    filename = f\"my_recording.wav\"\n",
        "    filepath = Path(\"./\") / filename # 儲存在目前工作目錄\n",
        "\n",
        "    try:\n",
        "        # 使用 wavio 儲存 WAV 檔案\n",
        "        wavio.write(str(filepath), data_int16, sample_rate, sampwidth=2) # sampwidth=2 for 16-bit\n",
        "        print(f\"錄音已儲存至：{filepath}\")\n",
        "        return str(filepath)\n",
        "    except Exception as e:\n",
        "        print(f\"儲存 WAV 檔案時發生錯誤：{e}\")\n",
        "        return f\"儲存錯誤：{e}\"\n",
        "\n",
        "# 建立 Gradio 介面\n",
        "# inputs: 麥克風錄音，回傳 numpy array (sample_rate, data)\n",
        "# outputs: 顯示檔案路徑 (gr.File) 和播放器 (gr.Audio)\n",
        "recorder_app = gr.Interface(\n",
        "    fn=save_recording,\n",
        "    inputs=gr.Audio(sources=[\"microphone\"], type=\"numpy\", label=\"點此錄音 (Click to Record)\"),\n",
        "    outputs=[\n",
        "        gr.File(label=\"儲存的 WAV 檔案 (Saved WAV File)\"),\n",
        "    ],\n",
        "    title=\"簡易錄音機 (Simple Audio Recorder)\",\n",
        "    description=\"錄製聲音並儲存為 WAV 檔案。錄音後，檔案路徑會顯示在下方，可供下載或複製路徑用於後續步驟。\",\n",
        "    allow_flagging=\"never\"\n",
        ")\n",
        "\n",
        "# 啟動 Gradio App\n",
        "# share=True 會產生公開連結，方便在 Colab/Kaggle 等環境使用\n",
        "# inline=True 會嘗試在 Notebook 中內嵌顯示 (不一定所有環境都支援)\n",
        "recorder_app.launch(share=True, inline=False) # 建議 share=True, inline=False"
      ],
      "metadata": {
        "id": "B5aAfkzzzuTH"
      },
      "id": "B5aAfkzzzuTH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(IPython.display.Audio(\"./my_recording.wav\"))"
      ],
      "metadata": {
        "id": "mawKYekn1N0u"
      },
      "id": "mawKYekn1N0u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 執行整合流程 - 範例 2: 聲音輸入 (若存在)，川普聲音\n",
        "print(\"--- 執行整合流程：範例 2 (聲音輸入) ---\")\n",
        "my_recording_wav_path = \"my_recording.wav\" # 假設已轉換成 WAV\n",
        "\n",
        "input_audio_for_test = None\n",
        "if Path(my_recording_wav_path).exists():\n",
        "    input_audio_for_test = my_recording_wav_path\n",
        "\n",
        "if input_audio_for_test:\n",
        "    print(f\"使用錄音檔: {input_audio_for_test}\")\n",
        "    try:\n",
        "        read_for_me(\n",
        "            s2t_model=s2t,\n",
        "            llm_model=llm,\n",
        "            transcripts_dict=transcripts,\n",
        "            input_audio_path=input_audio_for_test, # 使用錄音檔\n",
        "            voice_to_clone_key=\"trump.wav\",     # 模仿川普\n",
        "            output_file=\"my_recording_trump_voice.wav\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"執行範例 2 時發生錯誤: {e}\")\n",
        "else:\n",
        "    print(f\"找不到錄音檔 ({my_recording_wav_path} 或 {my_recording_webm_path})，跳過範例 2。\")\n",
        "    print(\"請確認已成功錄音並執行儲存步驟。\")"
      ],
      "metadata": {
        "id": "J00uOYsmzZOu"
      },
      "id": "J00uOYsmzZOu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c785dbf4",
      "metadata": {
        "id": "c785dbf4"
      },
      "source": [
        "## 整合：從輸入到輸出（解答）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2078a9d",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "c2078a9d"
      },
      "outputs": [],
      "source": [
        "def read_for_me(\n",
        "    s2t_model: AutomaticSpeechRecognitionPipeline,\n",
        "    llm_model: TextGenerationPipeline,\n",
        "    transcripts_dict: dict, # 改為必要參數\n",
        "    input_text: str | None = None,\n",
        "    input_audio_path: str | None = None,\n",
        "    voice_to_clone_key: str = \"spongebob.wav\", # 預設模仿海綿寶寶\n",
        "    output_file: str = \"final_output.wav\",\n",
        "    output_dir: str = \"tts_final\"\n",
        "):\n",
        "    \"\"\"整合 S2T -> LLM -> TTS 的流程。\"\"\"\n",
        "    # --- 1. 檢查與取得輸入文字 ---\n",
        "    if not input_text and not input_audio_path:\n",
        "        raise ValueError(\"必須提供 input_text 或 input_audio_path。\")\n",
        "    if input_text and input_audio_path:\n",
        "        raise ValueError(\"input_text 和 input_audio_path 不能同時提供。\")\n",
        "    if voice_to_clone_key not in transcripts_dict:\n",
        "        print(f\"警告：在文字稿字典中找不到 key '{voice_to_clone_key}'。\")\n",
        "        # raise ValueError(f\"在 transcripts_dict 中找不到 key: '{voice_to_clone_key}'\")\n",
        "\n",
        "    original_text = \"\"\n",
        "    if input_audio_path:\n",
        "        print(f\"步驟 1: S2T ({input_audio_path})\")\n",
        "        audio_file = Path(input_audio_path)\n",
        "        if not audio_file.exists(): raise FileNotFoundError(f\"找不到音檔: {input_audio_path}\")\n",
        "        original_text = s2t_model(str(audio_file))[\"text\"]\n",
        "        print(f\"  >> S2T 結果: {original_text}\")\n",
        "    else:\n",
        "        original_text = input_text\n",
        "        print(f\"步驟 1: 使用輸入文字: {original_text}\")\n",
        "\n",
        "    # --- 2. LLM 文法修正 ---\n",
        "    print(\"\\n步驟 2: LLM 文法修正\")\n",
        "    messages = prepare_grammar_correction_messages(original_text)\n",
        "    # 增加溫度參數讓輸出稍微多樣化，並設定停止符號 (如果模型支援)\n",
        "    llm_result = llm_model(messages, return_full_text=False, max_new_tokens=50)\n",
        "    corrected_text = llm_result[0][\"generated_text\"]\n",
        "    # 清理常見的模型輸出問題 (例如多餘的引號或標籤)\n",
        "    corrected_text = corrected_text.replace(\"```\", \"\").replace(\"`\", \"\").strip('\"').strip()\n",
        "    print(f\"  >> 修正後文字: {corrected_text}\")\n",
        "\n",
        "    # --- 3. TTS 聲音複製 ---\n",
        "    print(f\"\\n步驟 3: TTS 聲音複製 (模仿 {voice_to_clone_key})\")\n",
        "    voice_ref_text = transcripts_dict.get(voice_to_clone_key, {}) # 安全取得\n",
        "    voice_ref_path_str = f\"./week10/{voice_to_clone_key}\"\n",
        "    voice_ref_path = Path(voice_ref_path_str)\n",
        "\n",
        "    if not voice_ref_path.exists():\n",
        "        raise FileNotFoundError(f\"找不到參考聲音檔: {voice_ref_path_str}\")\n",
        "    if not voice_ref_text:\n",
        "         print(\"(警告：找不到參考聲音文字稿，ref_text 將為空)\")\n",
        "\n",
        "    clone_voice(\n",
        "        path_to_ref_audio=voice_ref_path,\n",
        "        gen_text=corrected_text,\n",
        "        ref_text=voice_ref_text,\n",
        "        output_file=output_file,\n",
        "        output_dir=output_dir\n",
        "    )\n",
        "    print(f\"\\n流程完成！最終語音已儲存至 {Path(output_dir) / output_file}\")"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "encoding": "# -*- coding: utf-8 -*-",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}